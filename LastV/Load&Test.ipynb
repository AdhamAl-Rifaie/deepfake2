{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9146200,"sourceType":"datasetVersion","datasetId":5524489},{"sourceId":10125851,"sourceType":"datasetVersion","datasetId":6248577},{"sourceId":5380830,"sourceType":"datasetVersion","datasetId":3120670},{"sourceId":7332880,"sourceType":"datasetVersion","datasetId":4256754},{"sourceId":673761,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":510623,"modelId":525314}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# CELL 1: IMPORT LIBRARIES\n# ============================================================\n\nprint(\"=\" * 60)\nprint(\"  IMPORTING LIBRARIES\")\nprint(\"=\" * 60)\n\n# Core Libraries\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\n\n# TensorFlow/Keras\nimport tensorflow as tf\nfrom tensorflow.keras.layers import (\n    Dense, Input, Layer, GlobalAveragePooling1D,\n    LayerNormalization, Lambda\n)\nfrom tensorflow.keras.models import Model\n\n# Set random seeds\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Check GPU\nprint(f\"\\nâœ… TensorFlow version: {tf.__version__}\")\nprint(f\"âœ… GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"âœ“ LIBRARIES LOADED\")\nprint(\"=\" * 60)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MdUOLB1qiYxt","outputId":"7050da6a-7db4-4498-e075-19b25559ddeb","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:31:36.880300Z","iopub.execute_input":"2025-12-05T22:31:36.880689Z","iopub.status.idle":"2025-12-05T22:31:36.892909Z","shell.execute_reply.started":"2025-12-05T22:31:36.880654Z","shell.execute_reply":"2025-12-05T22:31:36.891712Z"}},"outputs":[{"name":"stdout","text":"============================================================\n  IMPORTING LIBRARIES\n============================================================\n\nâœ… TensorFlow version: 2.18.0\nâœ… GPU Available: False\n\n============================================================\nâœ“ LIBRARIES LOADED\n============================================================\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ============================================================\n# CELL 2: MULTI-HEAD ATTENTION LAYER\n# ============================================================\n\nclass MultiHeadAttention(Layer):\n    \"\"\"Multi-Head Attention for spatial-temporal processing.\"\"\"\n\n    def __init__(self, num_heads, key_dim, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n        self.d_model = num_heads * key_dim\n\n        self.query_dense = Dense(self.d_model)\n        self.key_dense = Dense(self.d_model)\n        self.value_dense = Dense(self.d_model)\n        self.combine_heads = Dense(self.d_model)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.key_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, query, value, key):\n        batch_size = tf.shape(query)[0]\n\n        query = self.query_dense(query)\n        key = self.key_dense(key)\n        value = self.value_dense(value)\n\n        query = self.split_heads(query, batch_size)\n        key = self.split_heads(key, batch_size)\n        value = self.split_heads(value, batch_size)\n\n        matmul_qk = tf.matmul(query, key, transpose_b=True)\n        dk = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n        output = tf.matmul(attention_weights, value)\n\n        output = tf.transpose(output, perm=[0, 2, 1, 3])\n        output = tf.reshape(output, (batch_size, -1, self.d_model))\n\n        return self.combine_heads(output)\n\n    def get_config(self):\n        config = super(MultiHeadAttention, self).get_config()\n        config.update({\n            \"num_heads\": self.num_heads,\n            \"key_dim\": self.key_dim,\n        })\n        return config\n\nprint(\"âœ… MultiHeadAttention layer defined!\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6K55CDmVicm_","outputId":"a81c94e2-0a1e-4d30-cd2e-e18e024d7b1f","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:31:39.568865Z","iopub.execute_input":"2025-12-05T22:31:39.569441Z","iopub.status.idle":"2025-12-05T22:31:39.579294Z","shell.execute_reply.started":"2025-12-05T22:31:39.569416Z","shell.execute_reply":"2025-12-05T22:31:39.578386Z"}},"outputs":[{"name":"stdout","text":"âœ… MultiHeadAttention layer defined!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ============================================================\n# CELL 3: VISION TEMPORAL TRANSFORMER LAYER\n# ============================================================\n\nclass VisionTemporalTransformer(Layer):\n    \"\"\"Vision Temporal Transformer for video processing.\"\"\"\n\n    def __init__(self, patch_size=8, d_model=128, num_heads=4,\n                 spatial_layers=1, temporal_layers=1, **kwargs):\n        super(VisionTemporalTransformer, self).__init__(**kwargs)\n        self.patch_size = patch_size\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.spatial_layers = spatial_layers\n        self.temporal_layers = temporal_layers\n\n        self.dense_projection = Dense(d_model)\n        self.pos_emb = None\n\n        # Spatial transformer\n        self.spatial_mhas = [\n            MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)\n            for _ in range(spatial_layers)\n        ]\n        self.spatial_norm1 = [LayerNormalization() for _ in range(spatial_layers)]\n        self.spatial_ffn = [\n            tf.keras.Sequential([\n                Dense(d_model*4, activation='relu'),\n                Dense(d_model)\n            ]) for _ in range(spatial_layers)\n        ]\n        self.spatial_norm2 = [LayerNormalization() for _ in range(spatial_layers)]\n\n        # Temporal transformer\n        self.temporal_mhas = [\n            MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)\n            for _ in range(temporal_layers)\n        ]\n        self.temporal_norm1 = [LayerNormalization() for _ in range(temporal_layers)]\n        self.temporal_ffn = [\n            tf.keras.Sequential([\n                Dense(d_model*4, activation='relu'),\n                Dense(d_model)\n            ]) for _ in range(temporal_layers)\n        ]\n        self.temporal_norm2 = [LayerNormalization() for _ in range(temporal_layers)]\n\n    def build(self, input_shape):\n        H = input_shape[2]\n        W = input_shape[3]\n        ph = H // self.patch_size\n        pw = W // self.patch_size\n        num_patches = ph * pw\n\n        self.pos_emb = self.add_weight(\n            shape=(1, num_patches, self.d_model),\n            initializer='random_normal',\n            trainable=True,\n            name='pos_emb'\n        )\n        super(VisionTemporalTransformer, self).build(input_shape)\n\n    def call(self, inputs):\n        input_shape = inputs.get_shape()\n        shape = tf.shape(inputs)\n        batch = shape[0]\n        frames = shape[1]\n        H = shape[2]\n        W = shape[3]\n        C_static = input_shape[-1]\n        C = C_static if C_static is not None else shape[4]\n\n        reshaped = tf.reshape(inputs, [-1, H, W, C])\n\n        patches = tf.image.extract_patches(\n            images=reshaped,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding='VALID'\n        )\n\n        if C_static is not None:\n            patch_dim_static = self.patch_size * self.patch_size * C_static\n        else:\n            patch_dim_static = None\n\n        patch_dim_dynamic = tf.shape(patches)[-1]\n        final_patch_dim = patch_dim_static if patch_dim_static is not None else patch_dim_dynamic\n\n        patches = tf.reshape(patches, [-1, tf.shape(patches)[1] * tf.shape(patches)[2], final_patch_dim])\n\n        if patch_dim_static is not None:\n            patches.set_shape([None, None, patch_dim_static])\n\n        x = self.dense_projection(patches) + self.pos_emb\n\n        # Spatial transformer\n        for i in range(self.spatial_layers):\n            attn = self.spatial_mhas[i](x, x, x)\n            x = x + attn\n            x = self.spatial_norm1[i](x)\n            ff = self.spatial_ffn[i](x)\n            x = x + ff\n            x = self.spatial_norm2[i](x)\n\n        # Temporal pooling\n        x = tf.reshape(x, [batch, frames, -1, self.d_model])\n        x = tf.reduce_mean(x, axis=2)\n        x.set_shape([None, None, self.d_model])\n\n        # Temporal transformer\n        for i in range(self.temporal_layers):\n            attn = self.temporal_mhas[i](x, x, x)\n            x = x + attn\n            x = self.temporal_norm1[i](x)\n            ff = self.temporal_ffn[i](x)\n            x = x + ff\n            x = self.temporal_norm2[i](x)\n\n        pooled = GlobalAveragePooling1D()(x)\n        return pooled\n\n    def get_config(self):\n        config = super(VisionTemporalTransformer, self).get_config()\n        config.update({\n            \"patch_size\": self.patch_size,\n            \"d_model\": self.d_model,\n            \"num_heads\": self.num_heads,\n            \"spatial_layers\": self.spatial_layers,\n            \"temporal_layers\": self.temporal_layers,\n        })\n        return config\n\nprint(\"âœ… VisionTemporalTransformer layer defined!\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ba9pC8zEiehV","outputId":"ae9bcec5-8298-48b6-dc9a-9d5eeba73484","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:31:41.924842Z","iopub.execute_input":"2025-12-05T22:31:41.925723Z","iopub.status.idle":"2025-12-05T22:31:41.943388Z","shell.execute_reply.started":"2025-12-05T22:31:41.925695Z","shell.execute_reply":"2025-12-05T22:31:41.942300Z"}},"outputs":[{"name":"stdout","text":"âœ… VisionTemporalTransformer layer defined!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ============================================================\n# CELL 4: LOAD MODEL\n# ============================================================\n\nprint(\"=\" * 60)\nprint(\"  LOADING MODEL\")\nprint(\"=\" * 60)\n\nMODEL_PATH = '/kaggle/input/best-lipinc-model-h5/tensorflow2/default/1/best_lipinc_model.h5'\n\nprint(f\"\\nðŸ“‚ Model path: {MODEL_PATH}\")\n\n# Check if file exists\nif not os.path.exists(MODEL_PATH):\n    print(f\"\\nâŒ ERROR: Model not found!\")\n    print(f\"   Path: {MODEL_PATH}\")\n    raise FileNotFoundError(\"Model file not found\")\n\nfile_size_mb = os.path.getsize(MODEL_PATH) / (1024 * 1024)\nprint(f\"âœ… Model file found! ({file_size_mb:.2f} MB)\")\n\n# Rebuild architecture\nprint(f\"\\nðŸ—ï¸  Rebuilding model architecture...\")\n\nframe_input = Input(shape=(8, 64, 144, 3), name='FrameInput')\nresidue_input = Input(shape=(7, 64, 144, 3), name='ResidueInput')\n\nvt = VisionTemporalTransformer(\n    patch_size=8, d_model=128, num_heads=4,\n    spatial_layers=1, temporal_layers=1\n)\n\nframe_feat = vt(frame_input)\nresidue_feat = vt(residue_input)\n\nexpand1 = Lambda(lambda x: tf.expand_dims(x, axis=1), output_shape=(1, 128))\nq = expand1(frame_feat)\nk = expand1(residue_feat)\nv = k\n\nmha = MultiHeadAttention(num_heads=4, key_dim=32)\nattn_out = mha(q, value=v, key=k)\n\nsqueeze = Lambda(lambda x: tf.squeeze(x, axis=1), output_shape=(128,))\nattn_out = squeeze(attn_out)\n\nconcat = Lambda(lambda t: tf.concat(t, axis=1), output_shape=(384,))\nfusion = concat([frame_feat, residue_feat, attn_out])\n\nx = Dense(512, activation='relu')(fusion)\nx = Dense(256, activation='relu')(x)\nclass_output = Dense(2, activation='softmax', name='class_output')(x)\nfeatures_output = Dense(128, activation=None, name='features_output')(x)\n\nmodel = Model(\n    inputs=[frame_input, residue_input],\n    outputs=[class_output, features_output],\n    name='LIPINC_Model'\n)\n\nprint(\"âœ… Architecture rebuilt!\")\n\n# Load weights\nprint(f\"\\nðŸ“¥ Loading weights...\")\nmodel.load_weights(MODEL_PATH)\nprint(\"âœ… Weights loaded successfully!\")\n\nprint(f\"\\nðŸ“Š Model Info:\")\nprint(f\"   Total parameters: {model.count_params():,}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"âœ“ MODEL READY FOR INFERENCE\")\nprint(\"=\" * 60)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K7KuvGVoigmD","outputId":"246ebbae-59ec-4a69-b01b-7a5ad12e1c93","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:32:00.705942Z","iopub.execute_input":"2025-12-05T22:32:00.706977Z","iopub.status.idle":"2025-12-05T22:32:01.570939Z","shell.execute_reply.started":"2025-12-05T22:32:00.706936Z","shell.execute_reply":"2025-12-05T22:32:01.570075Z"}},"outputs":[{"name":"stdout","text":"============================================================\n  LOADING MODEL\n============================================================\n\nðŸ“‚ Model path: /kaggle/input/best-lipinc-model-h5/tensorflow2/default/1/best_lipinc_model.h5\nâœ… Model file found! (10.06 MB)\n\nðŸ—ï¸  Rebuilding model architecture...\nâœ… Architecture rebuilt!\n\nðŸ“¥ Loading weights...\nâœ… Weights loaded successfully!\n\nðŸ“Š Model Info:\n   Total parameters: 867,586\n\n============================================================\nâœ“ MODEL READY FOR INFERENCE\n============================================================\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# ============================================================\n# CELL 5: VIDEO PROCESSOR (SAME AS V6)\n# ============================================================\n\nclass VideoProcessor:\n    \"\"\"Video frame extraction with transition detection (V6 method).\"\"\"\n\n    def __init__(self, frame_count=8, dim=(64, 144), use_transitions=True):\n        self.frame_count = frame_count\n        self.dim = dim\n        self.use_transitions = use_transitions\n\n    def select_transition_frames(self, path):\n        \"\"\"Select frames at scene transitions.\"\"\"\n        cap = cv2.VideoCapture(path)\n        prev_gray = None\n        transition_scores = []\n        all_frames = []\n\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n            if prev_gray is not None:\n                diff = cv2.absdiff(gray, prev_gray)\n                mean_diff = np.mean(diff)\n                transition_scores.append(mean_diff)\n            else:\n                transition_scores.append(0)\n\n            all_frames.append(frame)\n            prev_gray = gray\n\n        cap.release()\n\n        if len(all_frames) >= self.frame_count:\n            top_indices = np.argsort(transition_scores)[-self.frame_count:]\n            top_indices = sorted(top_indices)\n        else:\n            top_indices = list(range(len(all_frames)))\n\n        frames = []\n        for idx in top_indices[:self.frame_count]:\n            frame = all_frames[idx]\n            frame = cv2.resize(frame, (self.dim[1], self.dim[0]))\n            frames.append(frame)\n\n        while len(frames) < self.frame_count:\n            frames.append(np.zeros((self.dim[0], self.dim[1], 3)))\n\n        return np.array(frames).astype(np.float32) / 255.0\n\n    def load_video(self, path):\n        \"\"\"Load video with transition detection.\"\"\"\n        return self.select_transition_frames(path)\n\n    def compute_residue(self, frames):\n        \"\"\"Compute frame differences.\"\"\"\n        residues = np.zeros((self.frame_count - 1, self.dim[0], self.dim[1], 3), dtype=np.float32)\n        if len(frames) > 1:\n            for i in range(1, len(frames)):\n                residues[i-1] = frames[i] - frames[i-1]\n        return residues\n\n# Initialize processor\nprocessor = VideoProcessor(frame_count=8, dim=(64, 144), use_transitions=True)\n\nprint(\"âœ… VideoProcessor initialized!\")\nprint(\"   Frame count: 8\")\nprint(\"   Dimensions: (64, 144)\")\nprint(\"   Method: Transition Detection (V6)\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"83MczG2dikpo","outputId":"622dbb97-25d4-4a68-e83d-423eec9e6bef","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:32:06.060480Z","iopub.execute_input":"2025-12-05T22:32:06.060839Z","iopub.status.idle":"2025-12-05T22:32:06.072524Z","shell.execute_reply.started":"2025-12-05T22:32:06.060814Z","shell.execute_reply":"2025-12-05T22:32:06.071271Z"}},"outputs":[{"name":"stdout","text":"âœ… VideoProcessor initialized!\n   Frame count: 8\n   Dimensions: (64, 144)\n   Method: Transition Detection (V6)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ============================================================\n# CELL 6: TEST SINGLE VIDEO (WITH VERIFICATION)\n# ============================================================\n\nprint(\"=\" * 60)\nprint(\"  TEST SINGLE VIDEO\")\nprint(\"=\" * 60)\n\n# -------------------- VIDEO PATH --------------------\nVIDEO_PATH = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences/01_03__podium_speech_happy__480LQD1C.mp4\"\n\n# Configuration\nCONFIDENCE_THRESHOLD = 0.75  # 75% threshold\n\nif not os.path.exists(VIDEO_PATH):\n    print(f\"\\nâŒ Video not found: {VIDEO_PATH}\")\nelse:\n    video_name = os.path.basename(VIDEO_PATH)\n    print(f\"\\nðŸ“¹ Video: {video_name}\")\n\n    # -------------------- LABEL DETECTION --------------------\n    path_lower = VIDEO_PATH.lower()\n\n    # From filename\n    if \"fake\" in video_name.lower():\n        filename_label = \"FAKE\"\n    elif \"real\" in video_name.lower():\n        filename_label = \"REAL\"\n    else:\n        filename_label = \"UNKNOWN\"\n\n    # From folder path\n    if \"/fake/\" in path_lower:\n        folder_label = \"FAKE\"\n    elif \"/real/\" in path_lower:\n        folder_label = \"REAL\"\n    else:\n        folder_label = \"UNKNOWN\"\n\n    # Expected label (prioritize folder)\n    if folder_label != \"UNKNOWN\":\n        expected_label = folder_label\n    elif filename_label != \"UNKNOWN\":\n        expected_label = filename_label\n    else:\n        expected_label = None\n\n    # Show detected labels\n    if expected_label:\n        print(f\"   Expected: {expected_label}\")\n\n        # Check for conflicts\n        if filename_label != \"UNKNOWN\" and folder_label != \"UNKNOWN\":\n            if filename_label != folder_label:\n                print(f\"   âš ï¸  WARNING: Filename says '{filename_label}' but folder is '{folder_label}'\")\n\n    # -------------------- PROCESS --------------------\n    print(f\"\\nðŸ”® Processing...\")\n\n    try:\n        # Load frames\n        frames = processor.load_video(VIDEO_PATH)\n        residues = processor.compute_residue(frames)\n\n        # Prepare input\n        frames_input = np.expand_dims(frames, axis=0)\n        residues_input = np.expand_dims(residues, axis=0)\n\n        # Predict\n        predictions = model.predict([frames_input, residues_input], verbose=0)\n        probs = predictions[0][0]\n\n        prob_real = probs[0]\n        prob_fake = probs[1]\n        prediction = \"REAL\" if prob_real > prob_fake else \"FAKE\"\n        confidence = max(prob_real, prob_fake)\n\n        # -------------------- RESULTS --------------------\n        print(\"\\n\" + \"=\" * 60)\n        print(\"  RESULTS\")\n        print(\"=\" * 60)\n\n        print(f\"\\nðŸ“Š Probabilities:\")\n        print(f\"   Real: {prob_real:.4f} ({prob_real*100:.2f}%)\")\n        print(f\"   Fake: {prob_fake:.4f} ({prob_fake*100:.2f}%)\")\n\n        print(f\"\\nðŸŽ¯ Prediction: {prediction}\")\n        print(f\"   Confidence: {confidence*100:.2f}%\")\n\n        # Check correctness\n        if expected_label:\n            correct = (prediction == expected_label)\n            print(f\"\\n   Expected: {expected_label}\")\n            print(f\"   Result: {'âœ… CORRECT' if correct else 'âŒ INCORRECT'}\")\n\n        # Confidence analysis\n        print(f\"\\nðŸ“ˆ Confidence Level:\")\n        if confidence > 0.95:\n            print(\"   ðŸŸ¢ Very High - Model is very certain\")\n        elif confidence > 0.85:\n            print(\"   ðŸŸ¢ High - Model is confident\")\n        elif confidence > 0.75:\n            print(\"   ðŸŸ¡ Medium - Acceptable confidence\")\n        elif confidence > 0.60:\n            print(\"   ðŸŸ  Low - Model is uncertain\")\n        else:\n            print(\"   ðŸ”´ Very Low - High uncertainty!\")\n\n        # Threshold check\n        if confidence < CONFIDENCE_THRESHOLD:\n            print(f\"\\n   âš ï¸  WARNING: Confidence below threshold ({CONFIDENCE_THRESHOLD*100:.0f}%)\")\n            print(f\"   Difference: {(prob_fake - prob_real)*100:.2f}%\")\n            print(f\"   Consider manual review or additional testing\")\n\n        # Ambiguous case\n        prob_diff = abs(prob_real - prob_fake)\n        if prob_diff < 0.15:  # Less than 15% difference\n            print(f\"\\n   âš ï¸  AMBIGUOUS: Probabilities are too close!\")\n            print(f\"   Difference: {prob_diff*100:.2f}%\")\n            print(f\"   Model cannot clearly distinguish\")\n\n        print(\"\\n\" + \"=\" * 60)\n\n    except Exception as e:\n        print(f\"\\nâŒ ERROR: {e}\")\n        import traceback\n        traceback.print_exc()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-aHb75Iinwl","outputId":"9be78a7c-2460-4df9-961b-d99a2ed886a2","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T23:00:38.618097Z","iopub.execute_input":"2025-12-05T23:00:38.618423Z","iopub.status.idle":"2025-12-05T23:00:51.911056Z","shell.execute_reply.started":"2025-12-05T23:00:38.618400Z","shell.execute_reply":"2025-12-05T23:00:51.910165Z"}},"outputs":[{"name":"stdout","text":"============================================================\n  TEST SINGLE VIDEO\n============================================================\n\nðŸ“¹ Video: 01_03__podium_speech_happy__480LQD1C.mp4\n\nðŸ”® Processing...\n\n============================================================\n  RESULTS\n============================================================\n\nðŸ“Š Probabilities:\n   Real: 0.2524 (25.24%)\n   Fake: 0.7476 (74.76%)\n\nðŸŽ¯ Prediction: FAKE\n   Confidence: 74.76%\n\nðŸ“ˆ Confidence Level:\n   ðŸŸ  Low - Model is uncertain\n\n   âš ï¸  WARNING: Confidence below threshold (75%)\n   Difference: 49.53%\n   Consider manual review or additional testing\n\n============================================================\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# ============================================================\n# CELL: CHECK TRAINING HISTORY\n# ============================================================\n\nimport pandas as pd\n\nHISTORY_PATH = '/content/drive/MyDrive/DeepFake Detection Project Material/Final Version/training_history.csv'\n\nif os.path.exists(HISTORY_PATH):\n    history_df = pd.read_csv(HISTORY_PATH)\n\n    print(\"=\" * 60)\n    print(\"  TRAINING HISTORY\")\n    print(\"=\" * 60)\n\n    # Best epoch\n    best_epoch = history_df['val_class_output_accuracy'].idxmax()\n\n    train_acc = history_df['class_output_accuracy'].iloc[best_epoch]\n    val_acc = history_df['val_class_output_accuracy'].iloc[best_epoch]\n\n    print(f\"\\nðŸ“Š Best Epoch ({best_epoch + 1}):\")\n    print(f\"   Train Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n    print(f\"   Val Accuracy:   {val_acc:.4f} ({val_acc*100:.2f}%)\")\n    print(f\"   Gap: {(train_acc - val_acc):.4f} ({(train_acc - val_acc)*100:.2f}%)\")\n\n    # Show all epochs\n    print(f\"\\nðŸ“ˆ Training Progress:\")\n    print(f\"{'Epoch':<8} {'Train Acc':<12} {'Val Acc':<12} {'Gap':<10}\")\n    print(\"=\" * 50)\n\n    for i in range(len(history_df)):\n        t_acc = history_df['class_output_accuracy'].iloc[i]\n        v_acc = history_df['val_class_output_accuracy'].iloc[i]\n        gap = t_acc - v_acc\n        marker = \" â† Best\" if i == best_epoch else \"\"\n        print(f\"{i+1:<8} {t_acc:.4f}      {v_acc:.4f}      {gap:+.4f}{marker}\")\n\nelse:\n    print(\"âŒ Training history not found!\")\n    print(f\"   Looking for: {HISTORY_PATH}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vKGwZ4ttlUhb","outputId":"c195f793-b0ff-4c14-c560-17463dd4624f"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","  TRAINING HISTORY\n","============================================================\n","\n","ðŸ“Š Best Epoch (13):\n","   Train Accuracy: 0.8485 (84.85%)\n","   Val Accuracy:   0.8435 (84.35%)\n","   Gap: 0.0050 (0.50%)\n","\n","ðŸ“ˆ Training Progress:\n","Epoch    Train Acc    Val Acc      Gap       \n","==================================================\n","1        0.5807      0.6919      -0.1112\n","2        0.6420      0.6968      -0.0548\n","3        0.7133      0.7677      -0.0544\n","4        0.7568      0.7628      -0.0060\n","5        0.7804      0.6748      +0.1056\n","6        0.7993      0.6553      +0.1440\n","7        0.8118      0.6822      +0.1297\n","8        0.8155      0.7482      +0.0673\n","9        0.8318      0.8362      -0.0044\n","10       0.8375      0.8289      +0.0087\n","11       0.8412      0.8264      +0.0148\n","12       0.8454      0.8337      +0.0116\n","13       0.8485      0.8435      +0.0050 â† Best\n","14       0.8532      0.8386      +0.0146\n","15       0.8595      0.8313      +0.0282\n","16       0.8574      0.8117      +0.0457\n","17       0.8643      0.8068      +0.0574\n","18       0.8643      0.7824      +0.0819\n","19       0.8684      0.8386      +0.0298\n","20       0.8695      0.8313      +0.0382\n","21       0.8742      0.8362      +0.0380\n","22       0.8789      0.8386      +0.0403\n","23       0.8779      0.8362      +0.0417\n"]}],"execution_count":42}]}